{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100134,"status":"ok","timestamp":1753384894773,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"},"user_tz":-480},"id":"NllZi1iYRxjo","outputId":"161f6b85-619f-420f-84c7-b55431844621"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loading Data\n","Loaded data shape: (525886, 5140)\n","HMA features: 440\n","Regime features: 364\n","Rolling features: 1300\n","Interaction features: 3027\n","Total features: 5139\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import warnings\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n","from sklearn.preprocessing import StandardScaler\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","import gc\n","warnings.filterwarnings('ignore')\n","\n","print(\"Loading Data\")\n","# Load the data with all features (HMA + regime + rolling + interactions)\n","train = pd.read_parquet('/content/drive/MyDrive/DRW Crypto Market Prediction/train_with_rolling_interactions.parquet')\n","print(f\"Loaded data shape: {train.shape}\")\n","\n","# Verify features are present\n","hma_features = [col for col in train.columns if 'hma_' in col]\n","regime_features = [col for col in train.columns if any(x in col for x in ['regime_', 'vol_cluster_', 'trend_', 'momentum_', 'market_'])]\n","rolling_features = [col for col in train.columns if 'rolling_' in col]\n","interaction_features = [col for col in train.columns if any(x in col for x in ['_div_', '_minus_', '_times_', '_spread', '_imbalance', '_velocity', '_acceleration'])]\n","\n","print(f\"HMA features: {len(hma_features)}\")\n","print(f\"Regime features: {len(regime_features)}\")\n","print(f\"Rolling features: {len(rolling_features)}\")\n","print(f\"Interaction features: {len(interaction_features)}\")\n","\n","# Get all feature columns (excluding target and time)\n","feature_cols = [col for col in train.columns if col not in ['time_id', 'label']]\n","print(f\"Total features: {len(feature_cols)}\")"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1753392823049,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"},"user_tz":-480},"id":"Wd_qeBaDZ9GU","outputId":"f805f905-d42f-45a6-8e7c-07f90d11580b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Time series CV initialized\n"]}],"source":["# custom time series CV class\n","\n","class WalkForwardTimeSeriesCV:\n","    \"\"\"\n","    Custom walk-forward time series cross-validation\n","    \"\"\"\n","    def __init__(self, n_splits=5, test_size=0.2, gap=0):\n","        self.n_splits = n_splits\n","        self.test_size = test_size\n","        self.gap = gap\n","\n","    def split(self, X, y=None):\n","        n_samples = len(X)\n","        test_size = int(n_samples * self.test_size)\n","\n","        for i in range(self.n_splits):\n","            # Calculate split points\n","            split_point = n_samples - (self.n_splits - i) * test_size\n","\n","            if split_point <= 0:\n","                continue\n","\n","            train_end = split_point - self.gap\n","            test_start = split_point\n","\n","            if train_end <= 0 or test_start >= n_samples:\n","                continue\n","\n","            train_indices = list(range(0, train_end))\n","            test_indices = list(range(test_start, min(test_start + test_size, n_samples)))\n","\n","            yield train_indices, test_indices\n","\n","    def get_n_splits(self, X, y=None):\n","        return self.n_splits\n","\n","# Initialize CV\n","tscv = WalkForwardTimeSeriesCV(n_splits=5, test_size=0.1, gap=100)\n","print(\"Time series CV initialized\")"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":57,"status":"ok","timestamp":1753392825071,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"},"user_tz":-480},"id":"zP55D-q3gio7"},"outputs":[],"source":["# feature selection functions\n","\n","def select_features_stability(X, y, n_features=500):\n","    \"\"\"\n","    Select features based on stability across CV folds\n","    \"\"\"\n","    print(\"Selecting features based on stability...\")\n","\n","    feature_scores = {}\n","    tscv = WalkForwardTimeSeriesCV(n_splits=3, test_size=0.1, gap=100)\n","\n","    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n","\n","        # Train LightGBM for feature importance\n","        model = lgb.LGBMRegressor(\n","            n_estimators=100,\n","            learning_rate=0.1,\n","            max_depth=6,\n","            random_state=42,\n","            verbose=-1\n","        )\n","\n","        model.fit(X_train, y_train)\n","\n","        # Get feature importance\n","        importance = model.feature_importances_\n","\n","        for i, feature in enumerate(X.columns):\n","            if feature not in feature_scores:\n","                feature_scores[feature] = []\n","            feature_scores[feature].append(importance[i])\n","\n","    # Calculate stability (lower std = more stable)\n","    feature_stability = {}\n","    for feature, scores in feature_scores.items():\n","        if len(scores) >= 2:\n","            stability_score = 1 / (1 + np.std(scores))  # Higher is better\n","            feature_stability[feature] = stability_score\n","\n","    # Select top features by stability\n","    sorted_features = sorted(feature_stability.items(), key=lambda x: x[1], reverse=True)\n","    selected_features = [f[0] for f in sorted_features[:n_features]]\n","\n","    print(f\"Selected {len(selected_features)} stable features\")\n","    return selected_features\n","\n","def select_features_correlation(X, y, threshold=0.95):\n","    \"\"\"\n","    Remove highly correlated features\n","    \"\"\"\n","    print(\"Removing highly correlated features...\")\n","\n","    # Calculate correlation matrix\n","    corr_matrix = X.corr().abs()\n","\n","    # Find highly correlated features\n","    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n","\n","    print(f\"Removed {len(to_drop)} highly correlated features\")\n","    return [col for col in X.columns if col not in to_drop]\n","\n","def select_features_importance(X, y, n_features=500):\n","    \"\"\"\n","    Select features based on LightGBM importance\n","    \"\"\"\n","    print(\"Selecting features based on importance...\")\n","\n","    # Train a quick model to get feature importance\n","    model = lgb.LGBMRegressor(\n","        n_estimators=100,\n","        learning_rate=0.1,\n","        max_depth=6,\n","        random_state=42,\n","        verbose=-1\n","    )\n","\n","    model.fit(X, y)\n","\n","    # Get feature importance\n","    importance = model.feature_importances_\n","    feature_importance = dict(zip(X.columns, importance))\n","\n","    # Select top features\n","    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n","    selected_features = [f[0] for f in sorted_features[:n_features]]\n","\n","    print(f\"Selected {len(selected_features)} important features\")\n","    return selected_features"]},{"cell_type":"code","source":["# main (# Step 1: Remove correlated features in chunks)\n","\n","X = train[feature_cols]\n","y = train['label']\n","print(f\"Starting with {X.shape[1]} features\")\n","\n","# Process correlation in smaller chunks\n","chunk_size = 500  # Smaller chunks for correlation\n","uncorr_features = []\n","\n","for i in range(0, len(feature_cols), chunk_size):\n","    chunk_features = feature_cols[i:i+chunk_size]\n","    print(f\"Processing correlation chunk {i//chunk_size + 1}: {len(chunk_features)} features\")\n","\n","    X_chunk = X[chunk_features]\n","\n","    # Calculate correlation within chunk\n","    corr_matrix = X_chunk.corr().abs()\n","    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","    to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\n","\n","    # Keep features not in to_drop\n","    keep_from_chunk = [f for f in chunk_features if f not in to_drop]\n","    uncorr_features.extend(keep_from_chunk)\n","\n","    print(f\"Kept {len(keep_from_chunk)} from {len(chunk_features)} features\")\n","\n","print(f\"After correlation removal: {len(uncorr_features)} features\")\n","\n","# Save intermediate result\n","import pickle\n","with open('uncorr_features.pkl', 'wb') as f:\n","    pickle.dump(uncorr_features, f)\n","\n","# Memory cleanup\n","del X, corr_matrix, upper_tri, X_chunk\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jlcPgvEN3zo4","executionInfo":{"status":"ok","timestamp":1753388211002,"user_tz":-480,"elapsed":3002767,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"}},"outputId":"fa59bdb8-f7bb-4013-ba84-bc45eff454d7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting with 5139 features\n","Processing correlation chunk 1: 500 features\n","Kept 286 from 500 features\n","Processing correlation chunk 2: 500 features\n","Kept 357 from 500 features\n","Processing correlation chunk 3: 500 features\n","Kept 239 from 500 features\n","Processing correlation chunk 4: 500 features\n","Kept 183 from 500 features\n","Processing correlation chunk 5: 500 features\n","Kept 336 from 500 features\n","Processing correlation chunk 6: 500 features\n","Kept 458 from 500 features\n","Processing correlation chunk 7: 500 features\n","Kept 428 from 500 features\n","Processing correlation chunk 8: 500 features\n","Kept 377 from 500 features\n","Processing correlation chunk 9: 500 features\n","Kept 353 from 500 features\n","Processing correlation chunk 10: 500 features\n","Kept 489 from 500 features\n","Processing correlation chunk 11: 139 features\n","Kept 101 from 139 features\n","After correlation removal: 3607 features\n"]},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# main (Step 2: Stability selection)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import warnings\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n","from sklearn.preprocessing import StandardScaler\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","import gc\n","warnings.filterwarnings('ignore')\n","\n","print(\"Reloading training data...\")\n","train = pd.read_parquet('/content/drive/MyDrive/DRW Crypto Market Prediction/train_with_rolling_interactions.parquet')\n","\n","# Load uncorrelated features\n","with open('uncorr_features.pkl', 'rb') as f:\n","    uncorr_features = pickle.load(f)\n","\n","print(f\"Processing {len(uncorr_features)} features in chunks\")\n","\n","# Process stability in chunks\n","chunk_size = 200  # Small chunks for stability\n","stable_candidates = []\n","y = train['label']\n","\n","for i in range(0, len(uncorr_features), chunk_size):\n","    chunk_features = uncorr_features[i:i+chunk_size]\n","    print(f\"Stability chunk {i//chunk_size + 1}/{(len(uncorr_features)-1)//chunk_size + 1}: {len(chunk_features)} features\")\n","\n","    try:\n","        X_chunk = train[chunk_features]\n","\n","        # Simple stability test - train 3 models on different data splits\n","        feature_scores = {}\n","\n","        for split in [0.3, 0.5, 0.7]:  # 3 different train sizes\n","            split_size = int(len(X_chunk) * split)\n","            X_split = X_chunk.iloc[:split_size]\n","            y_split = y.iloc[:split_size]\n","\n","            # Quick model\n","            model = lgb.LGBMRegressor(\n","                n_estimators=50,  # Reduced for speed\n","                learning_rate=0.1,\n","                max_depth=5,\n","                random_state=42,\n","                verbose=-1\n","            )\n","\n","            model.fit(X_split, y_split)\n","\n","            # Store importance\n","            for j, feature in enumerate(chunk_features):\n","                if feature not in feature_scores:\n","                    feature_scores[feature] = []\n","                feature_scores[feature].append(model.feature_importances_[j])\n","\n","        # Calculate stability for this chunk\n","        chunk_stable = []\n","        for feature, scores in feature_scores.items():\n","            if len(scores) >= 2:\n","                stability = 1 / (1 + np.std(scores))  # Higher = more stable\n","                chunk_stable.append((feature, stability))\n","\n","        # Take top 50% from each chunk\n","        chunk_stable.sort(key=lambda x: x[1], reverse=True)\n","        top_from_chunk = [f[0] for f in chunk_stable[:len(chunk_stable)//2]]\n","        stable_candidates.extend(top_from_chunk)\n","\n","        print(f\"  Added {len(top_from_chunk)} stable features from chunk\")\n","\n","        # Clean up\n","        del X_chunk, X_split, model\n","        gc.collect()\n","\n","    except Exception as e:\n","        print(f\"  Chunk failed: {e}, skipping...\")\n","\n","print(f\"Total stable candidates: {len(stable_candidates)}\")\n","\n","# Final selection - take top 400\n","if len(stable_candidates) > 400:\n","    # Quick final ranking\n","    X_final = train[stable_candidates]\n","    model = lgb.LGBMRegressor(n_estimators=50, random_state=42, verbose=-1)\n","    model.fit(X_final, y)\n","\n","    importance = dict(zip(stable_candidates, model.feature_importances_))\n","    sorted_final = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n","    stable_features = [f[0] for f in sorted_final[:400]]\n","else:\n","    stable_features = stable_candidates\n","\n","print(f\"Final stable features: {len(stable_features)}\")\n","\n","# Save result\n","with open('stable_features.pkl', 'wb') as f:\n","    pickle.dump(stable_features, f)\n","\n","print(\"Step 2 completed and saved!\")\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxVclXjd39Vg","executionInfo":{"status":"ok","timestamp":1753390528638,"user_tz":-480,"elapsed":473331,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"}},"outputId":"e500c8e3-7d40-44fb-f554-d686bdfff6c6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Reloading training data...\n","Processing 3607 features in chunks\n","Stability chunk 1/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 2/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 3/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 4/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 5/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 6/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 7/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 8/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 9/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 10/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 11/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 12/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 13/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 14/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 15/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 16/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 17/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 18/19: 200 features\n","  Added 100 stable features from chunk\n","Stability chunk 19/19: 7 features\n","  Added 3 stable features from chunk\n","Total stable candidates: 1803\n","Final stable features: 400\n","Step 2 completed and saved!\n"]},{"output_type":"execute_result","data":{"text/plain":["8"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# main (Step 3: Final importance selection)\n","\n","feature_cols = [col for col in train.columns if col not in ['time_id', 'label', 'index']]\n","# Load stable features\n","import pickle\n","with open('stable_features.pkl', 'rb') as f:\n","    stable_features = pickle.load(f)\n","\n","X_stable = train[stable_features]\n","y = train['label']\n","\n","print(f\"Running importance selection on {len(stable_features)} features\")\n","\n","# Final selection\n","final_features = select_features_importance(X_stable, y, n_features=500)\n","\n","print(f\"Final features: {len(final_features)}\")\n","\n","# Save selected features\n","selected_features_df = pd.DataFrame({'feature': final_features})\n","selected_features_df.to_csv('/content/drive/MyDrive/DRW Crypto Market Prediction/selected_features.csv', index=False)\n","\n","print(f\"\\n Feature selection completed!\")\n","print(f\"Original features: {len(feature_cols)}\")\n","print(f\"Final features: {len(final_features)}\")\n","print(\"Selected features saved to: selected_features.csv\")\n","\n","# Memory cleanup\n","del X_stable\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtV4-KQS4Chr","executionInfo":{"status":"ok","timestamp":1753390633748,"user_tz":-480,"elapsed":22087,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"}},"outputId":"0eee0baf-8ccc-4ea5-e2c6-af581010350e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Running importance selection on 400 features\n","Selecting features based on importance...\n","Selected 400 important features\n","Final features: 400\n","\n"," Feature selection completed!\n","Original features: 5139\n","Final features: 400\n","Selected features saved to: selected_features.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["42"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"r5S0RJu4hFM6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753391567808,"user_tz":-480,"elapsed":263860,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"}},"outputId":"6e2162b2-75a4-4502-8d6c-2d8ad048102d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data loaded: (525886, 5142)\n","Selected features: 400\n","X_final shape: (525886, 400)\n","y shape: (525886,)\n","\n","Fold 1/5\n","Fold 1 RMSE: 0.970242\n","\n","Fold 2/5\n","Fold 2 RMSE: 1.068473\n","\n","Fold 3/5\n","Fold 3 RMSE: 1.017032\n","\n","Fold 4/5\n","Fold 4 RMSE: 1.089529\n","\n","Fold 5/5\n","Fold 5 RMSE: 1.068730\n","\n"," CV results\n","Mean RMSE: 1.042801\n","Std RMSE: 0.043460\n","Min RMSE: 0.970242\n","Max RMSE: 1.089529\n","\n"," Cross-validation completed!\n"," CV results saved to: cv_results.csv\n"]}],"source":["\n","# Cross-validation with selected features\n","\n","import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import mean_squared_error\n","import gc\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Load train data\n","train = pd.read_parquet('/content/drive/MyDrive/DRW Crypto Market Prediction/train_with_rolling_interactions.parquet')\n","if 'time_id' not in train.columns:\n","    train = train.reset_index()\n","    train['time_id'] = range(len(train))\n","\n","# Load selected features\n","selected_features_df = pd.read_csv('/content/drive/MyDrive/DRW Crypto Market Prediction/selected_features.csv')\n","final_features = selected_features_df['feature'].tolist()\n","\n","print(f\"Data loaded: {train.shape}\")\n","print(f\"Selected features: {len(final_features)}\")\n","\n","X_final = train[final_features]\n","y = train['label']\n","\n","print(f\"X_final shape: {X_final.shape}\")\n","print(f\"y shape: {y.shape}\")\n","\n","cv_scores = []\n","feature_importance_cv = {}\n","\n","tscv = WalkForwardTimeSeriesCV(n_splits=5, test_size=0.1, gap=100)\n","\n","for fold, (train_idx, val_idx) in enumerate(tscv.split(X_final)):\n","    print(f\"\\nFold {fold + 1}/5\")\n","\n","    X_train, X_val = X_final.iloc[train_idx], X_final.iloc[val_idx]\n","    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n","\n","    # Train model\n","    model = lgb.LGBMRegressor(\n","        n_estimators=200,\n","        learning_rate=0.05,\n","        max_depth=8,\n","        num_leaves=31,\n","        random_state=42,\n","        verbose=-1\n","    )\n","\n","    model.fit(X_train, y_train)\n","\n","    # Predictions\n","    y_pred = model.predict(X_val)\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n","    cv_scores.append(rmse)\n","\n","    # Store feature importance\n","    for i, feature in enumerate(X_final.columns):\n","        if feature not in feature_importance_cv:\n","            feature_importance_cv[feature] = []\n","        feature_importance_cv[feature].append(model.feature_importances_[i])\n","\n","    print(f\"Fold {fold + 1} RMSE: {rmse:.6f}\")\n","\n","print(f\"\\n CV results\")\n","print(f\"Mean RMSE: {np.mean(cv_scores):.6f}\")\n","print(f\"Std RMSE: {np.std(cv_scores):.6f}\")\n","print(f\"Min RMSE: {np.min(cv_scores):.6f}\")\n","print(f\"Max RMSE: {np.max(cv_scores):.6f}\")\n","\n","# Save CV results\n","cv_results = pd.DataFrame({\n","    'fold': range(1, len(cv_scores) + 1),\n","    'rmse': cv_scores\n","})\n","cv_results.to_csv('/content/drive/MyDrive/DRW Crypto Market Prediction/cv_results.csv', index=False)\n","\n","print(\"\\n Cross-validation completed!\")\n","print(\" CV results saved to: cv_results.csv\")"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"hoSRTCIxhdM0","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1753393446011,"user_tz":-480,"elapsed":131125,"user":{"displayName":"Isabelle Lu","userId":"08270117842965788156"}},"outputId":"3c180703-02df-4435-bd8a-e011a245e4a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Created time_id from index: 0    0\n","1    1\n","2    2\n","3    3\n","4    4\n","Name: time_id, dtype: int64\n","Reloaded 400 selected features\n","Final dataset shape: (525886, 402)\n","\\ Final dataset created!\n","Shape: (525886, 402)\n"," Saved to: train_final_selected.parquet\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'X_final' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-137522795.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Memory cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_final' is not defined"]}],"source":["# Create final dataset with selected features\n","\n","import pandas as pd\n","train = pd.read_parquet('/content/drive/MyDrive/DRW Crypto Market Prediction/train_with_rolling_interactions.parquet')\n","\n","# Create time_id from the RangeIndex\n","train['time_id'] = train.index\n","print(f\"Created time_id from index: {train['time_id'].head()}\")\n","\n","selected_features_df = pd.read_csv('/content/drive/MyDrive/DRW Crypto Market Prediction/selected_features.csv')\n","final_features = selected_features_df['feature'].tolist()\n","\n","print(f\"Reloaded {len(final_features)} selected features\")\n","\n","# Add back target and time_id\n","final_dataset = train[['time_id', 'label'] + final_features]\n","print(f\"Final dataset shape: {final_dataset.shape}\")\n","\n","# Save final dataset\n","final_dataset.to_parquet('/content/drive/MyDrive/DRW Crypto Market Prediction/train_final_selected.parquet', index=False)\n","\n","print(f\"\\ Final dataset created!\")\n","print(f\"Shape: {final_dataset.shape}\")\n","print(\" Saved to: train_final_selected.parquet\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOTvyMjlobskVvZulDs4KoQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}